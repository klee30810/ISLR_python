{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What is statistical learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input variables : predictors, independent variables, features\n",
    "\n",
    "- output variables : response, depedent variables\n",
    "\n",
    "\n",
    "$$ Y = f(X) + \\epsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Why estimate f?\n",
    "\n",
    "Prediction \n",
    "---\n",
    "$ \\hat{f}(X) = \\hat{Y} $\n",
    "- reducible error : improve acc. by using the most appropriate statiscal learning technique\n",
    "- irreducible error \\epsilon : cannot be predicted using X\n",
    "  - may contain unmeasured varibles\n",
    "\n",
    "$$ \n",
    "E(T-\\hat(T))^2\\  =\\ E[f(X)+\\epsilon - \\hat{f}(X)]^2 = [f(X)-\\hat{f}]^2 + Var(\\epsilon)\n",
    "$$\n",
    "- The focus of this book is on techniques ofr estimating f with the aim of minimizing the reducible value\n",
    "\n",
    "Inference\n",
    "---\n",
    "- Which predictors are associated with the response?\n",
    "- What is the relationship between the response and each predictor?\n",
    "- Can the relationship between T and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 How do we estimate f?\n",
    "- training data : use observations to train, or teach, our method how to estimate f.\n",
    "- to apply a statistical learning method to the training data in order to estimate the unknown function f.\n",
    "\n",
    "Parametric Methods\n",
    "---\n",
    "1. Make an assumption about the functional form of f.\n",
    "2. Need a procedure that uses the training data to fit or train the model.\n",
    "\n",
    "- parametric : reduces the problem of estimating f down to one of estimating a set of parameters\n",
    "  - simplifies the problem because much easier to estimate a set of parameters\n",
    "  - the model we choose will usually not match the true unkonwn form of f\n",
    "  - More complex models can lead to a *overfitting* the data : follow noise too closely\n",
    "\n",
    "\n",
    "Non-parametric methods\n",
    "---\n",
    "- Do not make explicit assumptions about the functional form of f\n",
    "- seek an estimate of f that gets as close to the data points as possoble without being too rough : by avoiding the assumption of a particular functional form for f, potent to accurately fit a wider range of possible shapes for a.\n",
    "- since they do not reduce the problem of estimating f to a small number of parameters, a very large number of observations is required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Trade-off Between Prediction Accuracy and Model interpretablity\n",
    "- Restrictive models are much more interpretable.\n",
    "- Flexible models are more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Supervised VS Unsupervised Learning\n",
    "- supervised : for each observation of the predictor measurement, there is an associated response measurement\n",
    "- unsupervised : a measurements has no associated response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Regression VS Classification\n",
    "- quantitative variables : numerical values\n",
    "- qualitative variables : take on values in one of K different categories.\n",
    "- regression : problems with a quantitative response\n",
    "- classification : problems with a qualitative response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Assessing Model Accuracy\n",
    "### 2.2.1 Measuring the Quality of Fit\n",
    "- Mean Squared error : $MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i=\\hat{f}(x_i))^2 $\n",
    "  - interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data\n",
    "  - degrees of freedom : a function of flexibility -> restricted and smooth curve has fewer degrees of freedom\n",
    "\n",
    "### 2.2.2 The bias-variance trade-off\n",
    "- $ E(y_0-\\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon) $\n",
    "  - expected test MSE @ x_0 : average test MSE that we would obtain if we repeatedly estimated f using a larget number of training sets, and tested each @ x_0\n",
    "  - In order to minimize the expected test error, need to select a statistical learning method that simultaneously achieves low variance & low biase\n",
    "  - expected test MSE never lise below Var($\\epsilon$)\n",
    "- Variance : amount by which $\\hat{f}$ would change if we estimated it using a different training data set.\n",
    "- Bias : the error that is introduced by approximating a real-life problem by a much impler model.\n",
    "- More flexible methods, the variance will increaase and the bias will decrease.\n",
    "\n",
    "### 2.2.3 The cliassification setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kmlee')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d50b53f7eecc192699096c3b322d2e418f52a0f23d06ad75ff9fc286d66daf3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
