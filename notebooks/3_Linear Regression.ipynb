{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Regression\n",
    "- Is there a relationship between?\n",
    "- How strong is the relationship?\n",
    "- Which features are associated with the target?\n",
    "- How large the association?\n",
    "- How accurately can we predict target?\n",
    "- Is the relationship linear?\n",
    "- Is there synergy among features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.1 Simple linear regression\n",
    "- predicting a quantitative response Y on the basis of a single predictor X. assumes that there is approximately a linear relationship between X and Y.\n",
    "$$ Y \\approx \\beta_0 + \\beta_1X $$\n",
    "  - $ \\beta_0$ : intercept\n",
    "  - $ \\beta_1$ : slope\n",
    "\n",
    "### 3.1.1 Estimating the coefficients \n",
    "- to obtain coefficient estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such that the linear model fits the available data well.\n",
    "  - the most common approach involves minimizing the least squares criterion\n",
    "    $e_i = y_i - \\hat{y}_i$ : residual\n",
    "    $$ RSS = e^2_1 + e^2_2 + ... + e^2_n = (y_1-\\hat{\\beta}_0-\\hat{\\beta}_1x_1)^2 + (y_2-\\hat{\\beta}_0-\\hat{\\beta}_1x_2)^2 + ... + (y_n-\\hat{\\beta}_0-\\hat{\\beta}_1x_n)^2 \\\\ \\hat{\\beta_1} = \\frac{\\sum^n_{i=1}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum^n_{i=1}(x_i-\\bar{x})^2} \\\\ \\hat{\\beta_0}=\\bar{y}-\\hat{\\beta_1}\\bar{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Assessing the accuracy of the coefficient estimates\n",
    "$$ Y = \\beta_0 + \\beta_1X+\\epsilon $$\n",
    "- $\\epsilon$ : catch-all for what we miss with this simple model\n",
    "- The sample mean and the population mean are different, but in general hte smaple mean will provide a good estimate of the population mean.\n",
    "- If we use the sample mean $\\hat{\\mu}$ to estimate $\\mu$, this istemate is *unbiased*, in the sense that on average, we expect $\\hat{\\mu}$ to equal $\\mu$\n",
    "  - On the basis of one particular set of observations, $\\hat{\\mu}$ might overestimate of underestimate $\\mu$, then this average would exactly equal $\\mu$\n",
    "- How accurate is the sample mean $\\hat{\\mu}$ as an estimate of $\\mu$?\n",
    "  - standard error of $\\hat\\mu$ $$Var{\\hat\\mu}=SE(\\hat\\mu)^2=\\frac{\\sigma^2}{n} $$\n",
    "    - the average amount that his estimate $\\hat\\mu$ differs from the actual $\\mu$\n",
    "  $$ SE(\\hat\\beta_0)^2 = \\sigma^2[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum^n_{i=1}(x_i-\\bar{x})^2}], SE(\\hat\\beta_1)^2 = \\frac{\\sigma^2}{\\sum^n_{i=1}(x_i-\\bar{x})^2} $$\n",
    "    - SE is smaller when $x_i$ are more spread out; intuitiely we have more leverage to estimate a slope.\n",
    "  - In general, $\\sigma^2$ is not konwn, bur can be estimated from the data $RSE = \\sqrt{RSS/(n-2)}\n",
    "- Standard erros can be used to compute *confidence intervals*\n",
    "  - 95% confidence interval : a range of values such that with 95% probability, the range will contain the true unknown value of the parameter\n",
    "    - If we take repeated samples and construct the confidnece interval for each sample, 95% of the invercals will contain the true unknown value of the parameter\n",
    "    - $\\hat\\beta_1 \\pm 2 \\dot SE(\\hat\\beta_1)$\n",
    "- Standard errors can also be used to perform hypothesis tests on the coefficients\n",
    "  - $H_0$ : There is no relationship between X and Y, $\\beta_1=0$\n",
    "  - $H_1$ : There is some relationship between X and Y, $\\beta_0\\neq 0$\n",
    "  - need to determine whether $\\hat\\beta_1$ is sufficiently far from zero that we can confident that $\\beta$ is non-zero.\n",
    "    - depends on the accuracy of $\\hat\\beta_1$ which depends on SE($\\hat\\beta_1$) : if $\\hat\\beta_1$ is small, then even relatively small values of $\\hat\\beta_1$ may probide strong evidence that $\\beta_1 \\neq 0$. If SE($\\hat\\beta_1$) is large, $\\hat\\beta_1$ must be large in absolute value to reject the null hypothesis\n",
    "    - use *t-statistic* : $t = \\frac{\\hat\\beta_1-0}{SE(\\hat\\beta_1)} $=> measures standard deviation that \\hat\\beta_1 is away from 0.\n",
    "      - if there is no relationship, a t-distribution with n-2 degrees of freedom\n",
    "    - t-distribution has a bell shape and for values greather than 30 is similar to the stadard normal distribution.\n",
    "    - a small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to cahnce, in the absence of any real association between the predictor and the response => there is an association between the predictor and teh response. => **reject null hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Assessing the accuracy of the model\n",
    "- Want to quantify the extent to which the model fits the data => residual standard error (RSE) & $R^2$\n",
    "- Residual Standard Error : estimate of the standard deviation of $\\epsilon$\n",
    "  - actual target deviates from the regression line by RSE units, on average\n",
    "  - even if the model were correct and the coefficients were known exactly, prediction would still be off by RSE units on average.\n",
    "  - a measure of the lack of fit of the model to the data.\n",
    "- $R^2$ statistic : proportion of variance explained\n",
    "  - a value between 0 and 1 & independent of the scale of Y\n",
    "    $$ R^2 = \\frac{TSS-RSS}{TSS} = 1-\\frac{RSS}{TSS} \\\\ TSS = \\sum(y_i - \\bar{y})^2,\\ total\\ sum\\ of\\ squares$$\n",
    "  - TSS : amount of variability ingerent in the response before the regression is performed\n",
    "  - RSS : amount of variability that is left unexplained after performing regression.\n",
    "- correlation : measure of the linear relationship between X and Y\n",
    "  $$ Cor(X,Y) = \\frac{\\sum^n_{i=1}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum^n_{i=1}(x_i-\\bar{x})^2}\\sqrt{\\sum^n_{i=1}(y_i-\\bar{y})^2}}\n",
    "  - correlation quantifies the association between a single pair of variables rather than between a larger number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multiple linear regression\n",
    "- need to assess the relationship to the response with multiple predictors\n",
    "- separate simple linear regressions \n",
    "  - unclear how to make a single prediction of target given the three predictors since each is associated with a separate regression.\n",
    "  - each regression ignores other predictors for estimating coefficients.\n",
    "- extend the simple linear regression so that it can accomodate multiple predictors by giving each predictor a separate slope coefficient in a single model.\n",
    "  $$ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p+\\epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Estimating the regression coefficients\n",
    "- parameters are estimated with the same least squares approach to minimize the sum of squared residuals\n",
    "  $$ RSS = \\sum^n_{i=1}(y_i-\\hat{y_i})^2 = \\sum^n_{i=1}(y_i-\\hat\\beta_0-\\hat\\beta_1x_{i1}-\\hat\\beta_2x_{i2}-...-\\hat\\beta_px_{ip})^2\n",
    "- simple regression coefficient : represent relationship ignoring other predictors\n",
    "- multiple regression coefficient : represent relationship holding other predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2.2 Some important questions\n",
    "1. Is at least one of the predictors X1,X2, . . . ,Xp useful in predicting the response?\n",
    "2. Do all the predictors help to explain Y , or is only a subset of the predictors useful?\n",
    "3. How well does the model fit the data?\n",
    "4. Given a set of predictor values, what response value should we predict,and how accurate is our prediction?\n",
    "\n",
    "\n",
    "1) Is there a relationship between the response and predictors?\n",
    "- $$ H_0 : \\beta_1 = \\beta_2=...=\\beta_p=0 \\\\ H_1 : at\\ least\\ onw\\ \\beta_j\\ is\\ nonzero $$\n",
    "- computing F-statistic $ F=\\frac{(TSS-RSS)/p}{RSS/(n-p-1)} $\n",
    "  - linear model assumption correct : $E[RSS/(n-p-1)]=\\sigma^2$\n",
    "  - provided $H_0$ true : $E[(TSS-RSS)/p]=\\sigma^2 $ -> when there is no relationship between the response and predictors, F-statistic values close to 1(분자 분모 모두 분산). / If $H_a$ is true, then $E[(TSS-RSS)/p] > \\sigma^2$, F-statistic values larger than 1(분자가 시그마보다 커짐).\n",
    "- When n is large, an F-statistic just a little larger than 1 still provide evidence against $H_0$. / a larger F-statistic is needed to reject $H_0$ if n is small.\n",
    "- When $H_0$ is true and the errors $\\epsilon_i$ have a normal distribution, F-statistic follows an F-distribution. Compute the p-value associated with the F-statistic.\n",
    "- WHY DO WE NEED TO LOOK AT THE OVERALL F-STATISTIC when looking p-vals? If any one of p-values for the individual variables is very small, then *at least one of the predictors is related to the response*, but this is flawed, especially when the number of predictors p is large.\n",
    "  - If p=100 & $H_0$ is true, about 5% of p-values associated with each variable will be below 0.05 by chance. => **we expect to see approximately 5 small p-values even in the absence of any true association**\n",
    "  - F-statistic does not suffer from this : adjusts for the number of predictors\n",
    "- If p > n, we cannot fit the multiple linear regression model using least squares, so F-statistic cannot be used.\n",
    "\n",
    "2) Deciding on important variables\n",
    "- looking only individual p-values makes false discoveries.\n",
    "- variable selection : determine which predictors are associated with the response. -> $2^p$ models are needed. efficient approach to choose a smaller set of models to consider.\n",
    "  - forward selection : begin with null model, then fit p simple linear regressions and add to the null model the variable that results in the lowest RSS\n",
    "  - backward selection : begin with all variables, then remove the variable with the largest p-value. New (p-1) variable model is fit.\n",
    "  - mixed selection : begin with no variables, add the variable that provides the best fit. If at any point the p-value for one of the variables in the model rises above a certain threshold, then remove that varaible from the mdoel.\n",
    "\n",
    "3) Model fit\n",
    "- $R^2$ increases with added predictors despite no real improvement -> likely lead to poor results on independent test samples due to overfitting\n",
    "- non-linear pattern suggests a synergy or interaction effect between the predictors.\n",
    "\n",
    "4) Predictions\n",
    "- $ f(X) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p $\n",
    "- Due to irreducible error, we cannot perfectly predict. Prediction intervals are always wider than confidence intervals since they incorporate both the error in the estimate and the uncertainty as to how much individual point will differ from the popultion plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Other Considerations in the regression model\n",
    "### 3.3.1 Qualitative predictors\n",
    "- predictors with only two levels : 1,0 \n",
    "- predictors with more than two levels : create additional dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Extenstions of the linear model\n",
    "- linear model assumption : **relationship between the predictors and response are additive and linear**\n",
    "  - additivity : the association with predictor and response does not dpend on other predictors\n",
    "  - linearlity : the change in the response associated with a one-unit change in predictor is constant, regardless of the value of X\n",
    "- removing additive assumption\n",
    "  - add interaction term by computing product of $X_1$ & $X_2$\n",
    "  $$ Y = \\beta_0 + (\\beta_1 + \\beta_3X_2)X_1 + \\beta_2X_2 + \\epsilon \\\\ = \\beta_0 + \\bar{\\beta_1}X_1+\\beta_2X_2+\\epsilon\n",
    "  - \\bar{\\beta_1}$$ is now a function of $X_2$, the association between $X_1$ and $Y$ is no longer constant\n",
    "  - **Hierarchical principle : if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.**\n",
    "- Non-linear Relationships\n",
    "  - polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Potential problems\n",
    "1. Non-linearity of the data\n",
    "   - If the true relationship is far from lienar, then virtually all of the conclusions that we draw from the fit are suspect\n",
    "   - Residual plots (residuals vs predicted values): Ideally the residual plot will show no discernible pattern.\n",
    "2. Correlation of error terms.\n",
    "   - the error terms are uncorrelated : standard errors are based on the assumption of uncorrelated error terms.\n",
    "3. Non-constant variance of error terms.\n",
    "4. Outliers.\n",
    "5. High-leverage points.\n",
    "6. Collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kmlee')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d50b53f7eecc192699096c3b322d2e418f52a0f23d06ad75ff9fc286d66daf3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
