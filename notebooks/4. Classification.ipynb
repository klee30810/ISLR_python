{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Why not linear Regression\n",
    "- 정성적 변수 : 결과의 순서에 대한 순서가 존재함 ex. 경중, 중증, 위험군 환자\n",
    "  - 정량적 변수의 경우 1,2,3 등의 서로의 값들의 갭이 비슷하나, 정성적 변수의 경우, 1,2간의 갭이 1,3과의 갭 크기와 같지 않는 순서가 있다. 이런 갭을 선형회귀에서는 감안해줄 수가 없다.\n",
    "  - 이진 분류라면 dummy variable을 이용하여 처리할 수 (3.3.1) 있으나, 해석하기에 좀 난해한 단점도 존재\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Logistic regression\n",
    "- 로지스틱 회귀 : Y가 특별한 클래스에 속할 확률 모델링\n",
    "  \n",
    "### 4.3.1 The Logistic Model\n",
    "- $ p(X) = Pr(Y=1|X) $과 X의 관계 모델링 (이진분류 가정)\n",
    "- 선형 회귀 진행 시, 0,1을 벗어나는 값을 결과로 내놓는다. 그래서 (0, 1)의 범위를 가지는 logistic function을 적용한다. $$ p(X) = \\frac{e^{\\beta_0 +\\beta_1X}}{1+e^{\\beta_0+\\beta1X}}$$\n",
    "  - S-shaped 곡선의 형태를 가지며, 감지 가능한 예측 가능\n",
    "  - $$ \\frac{p(X)}{1-p(X)}=e^{\\beta_0+\\beta_1X}$$\n",
    "  - $ \\frac{p(X)}{1-p(X)} $ : odds(오즈) -> 분류에 있어서 확률 대신 사용됨\n",
    "  - \n",
    "- 해당 모델 적용을 위해 maximum likelihood 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Estimating the Regression Coefficients\n",
    "- maximum likelihood : $\\beta_0, \\beta_1$의 추정치는, 각 예상치 확률이, 각 추정치의 관측치와 최대한 가깝게 되도록\n",
    "- likelihood function\n",
    "  $$ l(\\beta_0,\\ \\beta_1) = \\prod_{i:y_i=1}p(x_i)\\prod_{i':y_{i'}=0}(1-p(x_{i'})) $$\n",
    "  : $\\beta_0,\\ \\beta_1$은 이 likelihood function이 최소회 되는 값을 선택한다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maximum Likelihood Estimation : 파라미터 $\\theta$로 구성된 어떤 확률밀도함수 $P(x|\\theta)$에서 관측된 표본 데이터$x=(x_1,...,x_n)$일 때 표본으로부터 파라미터 추정하는 방법\n",
    "  * likelihood : 지금 얻은 데이터가 이 분포로부터 나왔을 가능도. *각 데이터 샘플에서 후보 분포에 대한 높이(likelihood 기여도)를 계산해서 다 곱한 것* -> 곱하는 것은 모든 데이터 추출이 독립적이기 때문\n",
    "  $$ P(x|\\theta) = \\prod^n_{k=1}P(x_k|\\theta) \\\\ L(\\theta|x)=logP(x|\\theta) = \\sum^n_{i=1}logP(x_i|\\theta) $$\n",
    "  * 해당 likelihood function의 최대값 : 미분계수 이용. 찾고자 하는 파라미터에 대하여 편미분 후 0이 되도록 하는 파라미터를 찾기\n",
    "  $$\\frac{\\partial}{\\partial\\theta}L(\\theta|x)= \\frac{\\partial}{\\partial\\theta}logP(x|\\theta)=\\sum^n_{i=1}\\frac{\\partial}{\\partial\\theta}logP(x_i|\\theta)=0$$\n",
    "  * 평균 $\\mu$와 $\\sigma^2$를 모르는 정규분포를 가정하고, 여기서 추출한 값을 통해 모 분포의 평군과 분산 추정\n",
    "    * 정규 분포 가정 시, 표본분포는 $$f_{\\mu,\\sigma^2}(x_i)=\\frac{1}{\\sigma\\sqrt{2\\pi}}exp(-\\frac{(x_i-\\mu)^2}{2\\sigma^2})$$\n",
    "    * 독립추출 가정 $$ P(x|\\theta)=\\sum^n_{i=1}f_{\\mu,\\sigma^2}(x_i)=\\sum^n_{i=1}\\frac{1}{\\sigma\\sqrt{2\\pi}}exp(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}) $$\n",
    "    * 로그 우도 $$ L(\\theta|x)=\\sum^n_{i=1}log\\frac{1}{\\sigma\\sqrt{2\\pi}}exp(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}) = \\sum^n_{i=1}{log(exp(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}))-log(\\sigma\\sqrt{2\\pi})} = \\sum^n_{i=1}{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}-log(\\sigma)-log(\\sqrt{2\\pi})} $$\n",
    "     * $\\mu, \\sigma$로 편미분\n",
    "    $$ \\frac{\\partial L(\\theta|x)}{\\partial \\mu}=\\frac{1}{2\\sigma^2}\\sum^n_{i=1}\\frac{\\partial}{\\partial\\mu}(x^2_i-2x_i\\mu+\\mu^2)=-\\frac{1}{2\\sigma^2}\\sum^n_{i=1}(-2x_i+\\mu)=\\frac{1}{\\sigma^2}\\sum^n_{i=1}(x_i-\\mu)=\\frac{1}{\\sigma^2}(\\sum^n_{i=1}x_i-n\\mu)=0 \\\\ \\frac{\\partial L(\\theta|x)}{\\partial \\sigma} = -\\frac{n}{\\sigma}-\\frac{1}{2}\\sum^n_{i=1}(x_i-\\mu)^2\\frac{\\partial}{\\partial \\sigma}(\\frac{1}{\\sigma^2} = -\\frac{n}{\\sigma}+\\frac{1}{\\sigma^3}\\sum^n_{i=1}(x_i-\\mu)^2=0$$\n",
    "\n",
    "- reference : https://angeloyeo.github.io/2020/07/17/MLE.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Making Prediction\n",
    "$$ \\hat{p}(X)=\\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1}X}}{1+e^{\\hat{\\beta_0}+\\hat{\\beta_1}X}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Multiple Logistic Regression\n",
    "$$ \\hat{p}(X)=\\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1}X_1+...+\\hat{\\beta}_pX_p}}{1+e^{\\hat{\\beta_0}+\\hat{\\beta_1}X_1+...+\\hat{\\beta}_pX_p}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Multinomial Logistic Regression\n",
    "- 이분로지스틱 회귀는 2개 이상의 클래스로 확장될 수 있다 : 한 클래스를 베이스라인으로 선택하고, 일반성을 잃지 않는 선에서 K번째 클래스를 선택\n",
    "$$ Pr(Y=k|X=x) = \\frac{e^{\\beta_{k0}+\\beta_{k1}x_1+...+\\beta_{kp}pX_p}}{1+\\sum^{K-1}_{l=1}e^{\\beta_{l0}+\\beta_{l1}x_1+...+\\beta_{lp}pX_p}} $$\n",
    "- 다항로지스틱회귀는 베이스라인 클래스 선택에 따라 해석이 바뀔 수 있다\n",
    "- 다항 로직스틱회귀 대체 : softmax coding => 모든 K 클래스들을 대칭이 되게끔 취급한다\n",
    "  $$ Pr(Y=k|X=x) = \\frac{e^{\\beta_{k0}+\\beta_{k1}x_1+...+\\beta_{kp}pX_p}}{\\sum^{K}_{l=1}e^{\\beta_{l0}+\\beta_{l1}x_1+...+\\beta_{lp}pX_p}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Generative Models for Classification\n",
    "- logistic regression : 로지스틱 함수를 사용해 직접적으로 $Pr(Y=k|X=x)$(조건부 확률)를 모델링\n",
    "  - 각 종속변수 Y에 대한 독립변수 X의 분포를 모델링\n",
    "- Bayes' Theorem : 각 클래스에 대한 X의 정규분포로 가정하면, 모델은 로지스틱 회귀와 비슷해짐\n",
    "  - 이 방법이 필요한 이유\n",
    "    - 두 클래스 간 매우 큰 분리가 있을 경우, 로지스틱 회귀모델의 파라미터 예측은 매우 불안정\n",
    "    - 독립변수 분포가 정규분포이고 샘플 수가 적을경우, 이 접근법이 더 정확\n",
    "    - 이 방법은 2 이상 클래스의 방법으로 자연스럽게 확장\n",
    "  - $\\pi_k$ : prior -> k번째 클래스에 대한 임의로 선택된 관측치\n",
    "  - k번째 클래스의 밀도함수 : $f_k(X) \\equiv Pr(X|Y=k) =\\frac{\\pi_kf_k(x)}{\\sum^K_{l=1}\\pi_lf_l(x)}$\n",
    "    - posterior : x라는 관측이 k 클래스에 속할 확률.\n",
    "    - 직접 posterior를 구하는 것 보다 $\\pi_k, f_k(x)$의 예측치를 대입할 수 있다. 모집단에서 임의의 표본을 추출하는 경우 $pi_k$를 추정하는 것이 일반적으로 더 쉬운 방법이다.\n",
    "    - 하지만 $f_k(x)$는 더 어려운데, 몇 가지 가정이 필요하다\n",
    "  - $f_k(x)$ 예측치를 구할 수 있는 세 가지 방법을 볼 것 : linear discriminant analysis, quadratic discriminant anlysis, naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Linear Discriminant Analysis for p=1\n",
    "- p : predictor\n",
    "- $p_k(x)$를 구할 수 있는 $f_k(x)$를 구하고 $p_k(x)$가 가장 큰 클래스로 분류할 수 있다\n",
    "- $f_k(x)$를 정규분포로 가정, 1D일 경우. $$f_k(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{1}{2\\sigma^2_k}(x-\\mu_k)^2) $$\n",
    "- $\\sigma_1^2=...=\\sigma_K^2$ 가정 : $$ p_k(x)=\\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{1}{2\\sigma^2_k}(x-\\mu_k)^2)}{\\sum^K_{l=1}\\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{1}{2\\sigma^2_k}(x-\\mu_l)^2)}$$\n",
    "  - $\\pi$는 원주율이 아니라 관측치가 k 클래스로 분류될 prior 확률 임을 잊지 말기\n",
    "- 로그를 취하고 재정리 -> 관측치들을 $$ \\delta_k(x)=x\\times\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu^2_k}{2\\sigma^2}+log(\\pi_k)$$ 가 가장 큰 클래스로 분류하는 것\n",
    "- 실제로는 데이터가 정규분포에서 샘플링 되었다고 확신이 들어도, $\\mu, \\pi, \\sigma^2$는 예측해야 한다.\n",
    "- Linear Discriminant Analysis (LDA)는 구한 $\\mu, \\pi, \\sigma^2$를 $\\delta$식에 넣어서 베이지안 분류기를 근사한다 $$ \\hat{\\mu_k}=\\frac{1}{n_k}\\sum_{i:y_i=k}x_i\\\\ \\hat{\\sigma}^2=\\frac{1}{n-K}\\sum^K_{k=1}\\sum_{i:y_i=k}(x_i-\\hat{\\mu_k})^2$$\n",
    " - n :  모든 훈련 데이터 수  /  $n_k$ : k번째 클래스의 데이터 수   / $\\mu_k$ : k번째 클래스에 속하는 모든 관측치의 평균   / $\\hat{\\sigma^2}$: 각 클래스의 샘플 분산의 가중평균\n",
    "- 가끔 $\\pi_1, ..., \\pi_k$를 미리 알아서 직접적으로 사용가능한 경우 존재. 그런 데이터가 없으면 LDA에서는 k번째 클래스에 속하는 데이터 개수 비율을 통해서 $pi_k$를 예측한다 $$\\hat{\\pi_k}=n_k/n \\\\ \\hat{\\delta_k}(x)= x \\times\\frac{\\hat{\\mu_k}}{\\hat{\\sigma^2}}-\\frac{\\hat{\\mu_k}^2}{2\\hat{\\sigma^2}}+log(\\hat{\\pi_k})$$\n",
    "  - linear은 위의 식이 선형 함수라서 그렇다\n",
    "- LDA 분류기는 각 클래스가 정규분포이며, 클래스 별 특정 평균과 공통 분산을 가정하고, 해당 예측치를 베이지안 분류기에 대입한다.\n",
    "- 4.4.3에서 클래스별 특정 분산이 있는 경우를 다룬다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Linear Discriminant Anlaysis for p>1\n",
    "- 데이터는 다변량 정규분포를 가정하며, 클래스별 특정한 평균과 공통의 공분산을 가정한다\n",
    "  - 다변량 정규분포 : 각 독립변수가 1차원 정규분포를 따름, 각 독립변수 간 상관성 존재 -> 상관성이 있는 경우 bell shape가 타원형 \n",
    "  $$ X\\sim N(\\mu,N) \\\\ f(x)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)) \\\\ \\delta_k(x)=x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k+log\\pi_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kmlee')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d50b53f7eecc192699096c3b322d2e418f52a0f23d06ad75ff9fc286d66daf3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
