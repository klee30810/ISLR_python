{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear model을 Least square fitting이 아닌 다른 방법을 통해서 개선해보자\n",
    "  - 예측 정확도 : 독립, 종속변수가 선형이라고 가정할때, least square method는 낮은 bias일 것이다. 변수 종류보다 데이터수가 많다며 더 낮은 variance를 만들고 평가 정확도도 좋을 것이다.\n",
    "    - 만약 데이터의 수가 많지 않다면 least square fit에 variance가 많아져, 훈련되지 않은 평가 데이터셋에서 좋지 않은 결과를 만들 것이다. 만약 변수의 수가 데이터의 수보다 많다면 유일한 least square 계수가 없으므로 무한대의 Variance가 생긴다.\n",
    "    - 추정된 계수를 constrain or shrink를 통해서 사소한 bias의 증가와 함께 큰 variance의 감소를 만들 수 있으며, 평가 정확도를 크게 개선할 수 있다\n",
    "  - 모델 해석도 : 몇몇 독립변수들은 종속변수에 상관이 없을 수도 있고 이 들은 모델 복잡도만 크게 한다. 이런 변수를 지워서(해당 계수 0으로) 모델의 해석도를 높일 수 있다.\n",
    "    - least square는 어떤 계수 추정치를 0으로 만들지는 못한다. 변수 선택을 통해 관련 없는 독립변수를 제외해보자\n",
    "- least square 대안 들\n",
    "  - subset selection : p 독립변수에 대한 종속변수와 연관되어 있다고 생각되는 부분집합. 줄어든 부분집합 변수를 least square\n",
    "  - shrinkage : 모든 독립변수들을 모델 적합에 사용. least square 추정치에 따라서 추정계수가 0으로 수렴. 변수 선택에도 사용가능\n",
    "  - dimension reduction : p개의 변수들으 M차원 서브공간으로 투영하는 것. M개의 다른 선형 조합 or 투영 진행. M개의 투영들이 least square에 의해 선형 회귀모델 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Subset Selection\n",
    "### 6.1.1 Best Subset Selection\n",
    "- best subset selection : p개의 독립변수에 대한 가능한 각 조합에 least square 회귀 진행 -> p개의 하나의 독립변수만 가진 모델, $\\binom{p}{2}=p(p-1)/2$개의 2개의 독립변수를 가진 모델들 etc & 어떤 것이 제일 나은 모델인지 확인\n",
    "  - $2^p$개의 가능성이 있어서 개수가 너무 많다\n",
    "> Algorithm 6.1 Best Subset Selection\n",
    "> 1. $M_0$를 null model로 가정(아무 독립변수 없음, 각 관측에 대한 표본 평균만 예측)\n",
    "> 2. $k=1,2,...,p:\\\\$\n",
    ">  a) 모든 k개의 독립변수를 포함하는 $\\binom{p}{k}$를 적합  $\\\\$\n",
    ">  b) 이중 가장 낮은 RSS or 높은 $R^2$를 가진 모델을 $M_k$라고 지정\n",
    "> 3. Cross validation, AIC, BIC, adjusted $R^2$등을 통해서 가장 좋은 모델을 확인\n",
    "  - $R^2$는 독립변수의 수가 늘어나면 자연스럽게 늘게 되는데 이건 훈련 오차이다. 훈련오차는 평가 오차보다 작을 수 밖에 없으며, 훈련 오차가 작은 것이 평가 오차가 작은 것을 보장하지않으므로, CV, $C_p$, BIC, adjusted $R^2$ 등을 이용한다.\n",
    "  - 다른 회귀에도 사용가능 : logistic regression에서는 RSS 대신에 deviance($-2 * maximized\\ log-likelihood$)를 사용 \n",
    "  - 독립변수 증가에 따라 계산량이 너무 많아짐 ($2^p$) -> 변수가 40개가 넘어가면 못함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Stepwise Selection\n",
    "- 찾아야 되는 공간이 클 수록, 훈련 데이터에만 적합한 모델을 찾을 가능성이 높아진다. => 더 제한된 데이터셋에서 적합하기\n",
    "- Forward Stepwise Selection\n",
    "  - 같은 $2^p$의 가능성을 가지나 더 적은 셋\n",
    "  - 변수가 없는 모델에서 시작해서 한번에 하나씩 더하여 모든 변수가 있는 모델까지 적합\n",
    "  - 매 단계에서 적합에 추가적인 이득을 주는 변수만 포함한다\n",
    "> Algorithm 6.2 Forward stepwise selection\n",
    "> 1. $M_0$를 null model로 가정(아무 독립변수 없음, 각 관측에 대한 표본 평균만 예측)\n",
    "> 2. $k=0,1,...,p-1:\\\\$\n",
    ">  a) 한 개의 추가적인 변수가 있는 $M_k$가 증가된 p-k개의 모든 모델을 고려 $\\\\$\n",
    ">  b) p-k개의 모델 중 가장 낮은 RSS or 높은 $R^2$를 가진 모델을 $M_{k+1}$라고 지정\n",
    "> 3. Cross validation, AIC, BIC, adjusted $R^2$등을 통해서 가장 좋은 모델을 확인\n",
    "  - 하나의 null model에서 k번째 반복에 해당되는 p-k개의 모델을 적합 : $\\sum^{p-1}_{k=0}(p=k)=1+p(p+1)/2$\n",
    "  - 실패 가능 ex. 일변수 모델에서는 첫 번째, 이변수 모델에서는 두 세번째 변수가 채택되었을때, 이변수 모델에서는 첫 번재 변수가 무조건 선택되어야 하므로, 두 세번째 변수만 포함 된 모델을 선택할 수 없다\n",
    "  - 변수(p)가 데이터(n)보다 많은 고차원 데이터에도 적용될 수 있으나,least square를 사용해서 $M_0,...,M_{n-1}$ n개의 부분모델 밖에서 만들 수 없다\n",
    "- Backward Stepwise Selection\n",
    "  - p 변수 모두 포함된 least square 모델에서 반복적으로 하나씩 가장 무의미한 변수를 지워나감\n",
    "> Algorithm 6.3 Backward stepwise selection\n",
    "> 1. $M_p$를 full model로 가정(모든 변수 가짐)\n",
    "> 2. $k=p,p-1,...,1:\\\\$\n",
    ">  a) k-1개의 모든 변수들 중, $M_k$의 모든 변수들 중 하나 만 없는 모든 k개의 모델을 고려  $\\\\$\n",
    ">  b) k개의 모델 중 가장 낮은 RSS or 높은 $R^2$를 가진 모델을 $M_{k-1}$라고 지정\n",
    "> 3. Cross validation, AIC, BIC, adjusted $R^2$등을 통해서 가장 좋은 모델을 확인\n",
    "  - Backward selection도 1+p(p+1)/2개의 모델을 가져, p가 매우 커도 적용이 가능\n",
    "  - forward처럼 backward도 최고의 조합을 보장하지는 않는다.\n",
    "  - backward는 데이터의 수가 변수의 수 보다 많아야 하나(full model이 적합되어야 하므로), forward는 데이터의 수가 더 적어도 적용가능\n",
    "- Hybrid Approaches\n",
    "  - 순차적으로 변수를 추가한 후, 적합에 기여하지 않는 변수는 지워나감."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Choosing the Optimal Model\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
