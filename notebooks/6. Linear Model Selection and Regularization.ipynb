{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear model을 Least square fitting이 아닌 다른 방법을 통해서 개선해보자\n",
    "  - 예측 정확도 : 독립, 종속변수가 선형이라고 가정할때, least square method는 낮은 bias일 것이다. 변수 종류보다 데이터수가 많다며 더 낮은 variance를 만들고 평가 정확도도 좋을 것이다.\n",
    "    - 만약 데이터의 수가 많지 않다면 least square fit에 variance가 많아져, 훈련되지 않은 평가 데이터셋에서 좋지 않은 결과를 만들 것이다. 만약 변수의 수가 데이터의 수보다 많다면 유일한 least square 계수가 없으므로 무한대의 Variance가 생긴다.\n",
    "    - 추정된 계수를 constrain or shrink를 통해서 사소한 bias의 증가와 함께 큰 variance의 감소를 만들 수 있으며, 평가 정확도를 크게 개선할 수 있다\n",
    "  - 모델 해석도 : 몇몇 독립변수들은 종속변수에 상관이 없을 수도 있고 이 들은 모델 복잡도만 크게 한다. 이런 변수를 지워서(해당 계수 0으로) 모델의 해석도를 높일 수 있다.\n",
    "    - least square는 어떤 계수 추정치를 0으로 만들지는 못한다. 변수 선택을 통해 관련 없는 독립변수를 제외해보자\n",
    "- least square 대안 들\n",
    "  - subset selection : p 독립변수에 대한 종속변수와 연관되어 있다고 생각되는 부분집합. 줄어든 부분집합 변수를 least square\n",
    "  - shrinkage : 모든 독립변수들을 모델 적합에 사용. least square 추정치에 따라서 추정계수가 0으로 수렴. 변수 선택에도 사용가능\n",
    "  - dimension reduction : p개의 변수들으 M차원 서브공간으로 투영하는 것. M개의 다른 선형 조합 or 투영 진행. M개의 투영들이 least square에 의해 선형 회귀모델 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Subset Selection\n",
    "### 6.1.1 Best Subset Selection\n",
    "- best subset selection : p개의 독립변수에 대한 가능한 각 조합에 least square 회귀 진행 -> p개의 하나의 독립변수만 가진 모델, $\\binom{p}{2}=p(p-1)/2$개의 2개의 독립변수를 가진 모델들 etc & 어떤 것이 제일 나은 모델인지 확인\n",
    "  - $2^p$개의 가능성이 있어서 개수가 너무 많다\n",
    "> Algorithm 6.1 Best Subset Selection\n",
    "> 1. $M_0$를 null model로 가정(아무 독립변수 없음, 각 관측에 대한 표본 평균만 예측)\n",
    "> 2. $k=1,2,...,p:\\\\$\n",
    ">  a) 모든 k개의 독립변수를 포함하는 $\\binom{p}{k}$를 적합  $\\\\$\n",
    ">  b) 이중 가장 낮은 RSS or 높은 $R^2$를 가진 모델을 $M_k$라고 지정\n",
    "> 3. Cross validation, AIC, BIC, adjusted $R^2$등을 통해서 가장 좋은 모델을 확인\n",
    "  - $R^2$는 독립변수의 수가 늘어나면 자연스럽게 늘게 되는데 이건 훈련 오차이다. 훈련오차는 평가 오차보다 작을 수 밖에 없으며, 훈련 오차가 작은 것이 평가 오차가 작은 것을 보장하지않으므로, CV, $C_p$, BIC, adjusted $R^2$ 등을 이용한다.\n",
    "  - 다른 회귀에도 사용가능 : logistic regression에서는 RSS 대신에 deviance($-2 * maximized\\ log-likelihood$)를 사용 \n",
    "  - 독립변수 증가에 따라 계산량이 너무 많아짐 ($2^p$) -> 변수가 40개가 넘어가면 못함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Stepwise Selection\n",
    "- 찾아야 되는 공간이 클 수록, 훈련 데이터에만 적합한 모델을 찾을 가능성이 높아진다. => 더 제한된 데이터셋에서 적합하기\n",
    "- Forward Stepwise Selection\n",
    "  - 같은 $2^p$의 가능성을 가지나 더 적은 셋\n",
    "  - 변수가 없는 모델에서 시작해서 한번에 하나씩 더하여 모든 변수가 있는 모델까지 적합\n",
    "  - 매 단계에서 적합에 추가적인 이득을 주는 변수만 포함한다\n",
    "> Algorithm 6.2 Forward stepwise selection\n",
    "> 1. $M_0$를 null model로 가정(아무 독립변수 없음, 각 관측에 대한 표본 평균만 예측)\n",
    "> 2. $k=0,1,...,p-1:\\\\$\n",
    ">  a) 한 개의 추가적인 변수가 있는 $M_k$가 증가된 p-k개의 모든 모델을 고려 $\\\\$\n",
    ">  b) p-k개의 모델 중 가장 낮은 RSS or 높은 $R^2$를 가진 모델을 $M_{k+1}$라고 지정\n",
    "> 3. Cross validation, AIC, BIC, adjusted $R^2$등을 통해서 가장 좋은 모델을 확인\n",
    "  - 하나의 null model에서 k번째 반복에 해당되는 p-k개의 모델을 적합 : $\\sum^{p-1}_{k=0}(p=k)=1+p(p+1)/2$\n",
    "  - 실패 가능 ex. 일변수 모델에서는 첫 번째, 이변수 모델에서는 두 세번째 변수가 채택되었을때, 이변수 모델에서는 첫 번재 변수가 무조건 선택되어야 하므로, 두 세번째 변수만 포함 된 모델을 선택할 수 없다\n",
    "  - 변수(p)가 데이터(n)보다 많은 고차원 데이터에도 적용될 수 있으나,least square를 사용해서 $M_0,...,M_{n-1}$ n개의 부분모델 밖에서 만들 수 없다\n",
    "- Backward Stepwise Selection\n",
    "  - p 변수 모두 포함된 least square 모델에서 반복적으로 하나씩 가장 무의미한 변수를 지워나감\n",
    "> Algorithm 6.3 Backward stepwise selection\n",
    "> 1. $M_p$를 full model로 가정(모든 변수 가짐)\n",
    "> 2. $k=p,p-1,...,1:\\\\$\n",
    ">  a) k-1개의 모든 변수들 중, $M_k$의 모든 변수들 중 하나 만 없는 모든 k개의 모델을 고려  $\\\\$\n",
    ">  b) k개의 모델 중 가장 낮은 RSS or 높은 $R^2$를 가진 모델을 $M_{k-1}$라고 지정\n",
    "> 3. Cross validation, AIC, BIC, adjusted $R^2$등을 통해서 가장 좋은 모델을 확인\n",
    "  - Backward selection도 1+p(p+1)/2개의 모델을 가져, p가 매우 커도 적용이 가능\n",
    "  - forward처럼 backward도 최고의 조합을 보장하지는 않는다.\n",
    "  - backward는 데이터의 수가 변수의 수 보다 많아야 하나(full model이 적합되어야 하므로), forward는 데이터의 수가 더 적어도 적용가능\n",
    "- Hybrid Approaches\n",
    "  - 순차적으로 변수를 추가한 후, 적합에 기여하지 않는 변수는 지워나감."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Choosing the Optimal Model\n",
    "- 모든 독립변수를 가진 모델이 당연히 좋은 RSS, $R^2$를 가질 것이고, 이는 훈련 오차와 관련된 값이라 좋은 모델 평가값이라고 할 수 없다\n",
    "  1) 오버피팅으로 인해 bias를 설명할 수 있는 훈련오차에 보정을 만듬으로써 간접적으로 평가 오차 추정\n",
    "  2) 검증 세트나, cross-validation을 통해 평가 오차를 직접적으로 적용\n",
    "- $C_p$, AIC, BIC, and Adjusted $R^2$\n",
    "  - MSE : least square를 통한 훈련 데이터 적합으로 인해 훈련 데이터 MSE는 평가 데이터 MSE의 과소평가 버전이며, 변수가 증가할 수록 훈련 오차는 줄어드는데 평가 오차는 늘 수 있어 훈련 데이터에 대한 RSS, $R^2$는 평가에 이용될 수 없음\n",
    "  - 모델 사이즈에 대한 훈련 오차를 보정하는 여러 기법들\n",
    "    - $C_p$ : $2d\\hat\\sigma^2$의 페널티를 훈련 RSS에 더함으로서, 훈련 오차가 평가 오차에 대한 과소평가 되는 부분을 보완 $$C_p=\\frac{1}{n}(RSS+2d\\hat\\sigma^2),\\quad \\hat\\sigma^2:error\\ \\epsilon 에\\ 대한\\ 분산\\ 추정치(보통\\ full\\ model) $$\n",
    "      - 변수의 수가 늘어나면 페널티도 늘어나면서 훈련 오차 RSS 감소 보정\n",
    "    - AIC (Akaike Information Criterion) : 최대우도적합을 통한 큰 클래스 모델 적합 $$AIC = \\frac{1}{n}(RSS+2d\\hat\\sigma^2) $$\n",
    "      - $C_p$와 정비례\n",
    "    - BIC (Bayesian Information Criterion) : 베이지안 관점 해설 $$BIC=\\frac{1}{n}(RSS+log(n)d\\hat\\sigma^2) $$\n",
    "      - 모든 n>7에 대해 logn>2 이므로, BIC는 많은 변수가 있는 모델에 더 큰 페널티를 주어 $C_p$보다 더 작은 모델들을 결과로 만듬\n",
    "    - Adjusted $R^2$ : 모든 옳은 변수가 모델에 포함 될 시, 추가적인 노이즈 변수는 약간의 RSS 감소를 만듬. $$ Adjusted\\ R^2=1-\\frac{RSS/(n-d-1)}{TSS/(n-1)}$$\n",
    "      - 노이즈 변수 추가는 d를 증가시켜, adjusted $R^2$를 줄임\n",
    "- Validation and Cross-Validation\n",
    "  - 검증 데이터 셋을 통한 추정 평가 결과로 모델 평가\n",
    "  - one-standard-error rule : 오차가 비슷한 모델 중에서는 변수가 적은 모델이 좋다\n",
    "  1) 각 모델에 대한 추정 평가 MSE의 표준오차 계산\n",
    "  2) 표준오차 1 수준내 커브의 가장 낮은 포인트에서 추정 평가오차가 가장 낮은 모델 선택\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Shrinkage Methods\n",
    "- 모든 변수를 사용하고, 추정 계수를 제한하거나 0으로 줄여서 적합\n",
    "- 적합도 향상에 명확하지는 않지만 매우 급격히 variance를 줄여준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Ridge Regression\n",
    "$$ RSS = \\sum^n_{i=1}(y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij})^2$$\n",
    "- Ridge Regression : least square와 비슷한데, 계수에 약간의 차이를 최소화 함\n",
    "  - $\\hat\\beta^R$는 아래 식을 최소화 하는 값 $$ \\sum^n_{i=1}(y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij})^2+\\lambda\\sum^p_{j=1}\\beta^2_j=RSS+\\lambda\\sum^p_{j=1}\\beta^2_j, \\\\ \\lambda:tuning\\ parameter$$\n",
    "  - RSS를 최소하하여 데이터 적합을 시킴과 동시에 shirinkage penalty $\\lambda\\sum_j\\beta^2_j$는 각 계수가 0일 때 작아져, 각 계수를 0으로 수렴시킴 \n",
    "  - $\\lambda$는 회귀 계수 추정에 있어 두 항의 상대적 영향력 결정\n",
    "  - $\\lambda$ 선택이 중요\n",
    "  - shrinkage penalty는 intercept에 적용되지는 않음(모든 변수가 0일 때 평균)\n",
    "  - $\\lambda$가 클 수록 모든 리지 계수가 0으로 수렴하면서, 변수가 없는 null model화가 됨\n",
    "  - $||\\hat\\beta^R_2||_2/||\\hat\\beta||_2,\\ ||\\beta||_2=\\sqrt{\\sum^p_{j=1}\\beta^2_j}:l2\\ norm,$ 0~$\\beta$거리\n",
    "    - $\\lambda$가 커질 수록 $\\hat\\beta^R_\\lambda$의 l2 norm은 줄어들며, $||\\hat\\beta^R_2||_2/||\\hat\\beta||_2$도 줄어들음\n",
    "    - $||\\hat\\beta^R_2||_2/||\\hat\\beta||_2$이 1일때 $\\lambda$=0(리지 회귀 계수가 least square 추정치와 같고, l2 norm도 같음), $||\\hat\\beta^R_2||_2/||\\hat\\beta||_2$이 0일때 $\\lambda=\\inf$(리지 회귀 계수가 0벡터, l2 norm도 0)\n",
    "- 표준 최소제곱 계수 추정치는 scale equivariant : $X_j$를 c만큼 곱하면, 최소제곱계수 추정치가 1/c만큼 스케일링 되어 $X_j\\hat\\beta_j$는 동일하다\n",
    "- 반면에 리지 회귀 추정치는 상수를 각 변수에 곱해주었을 때 급격하게 변할 수 있다 => $X_j\\hat\\beta^R_{j,\\lambda}$는 $\\lambda$ 뿐만 아니라 j번ㅉ ㅐ변수의 스케일링에도 의존한다. $X_j\\hat\\beta^R_{j,\\lambda}$는 다른 변수의 스케일링에도 의존할 수 있어서 ridge regression은 변수 표준화를 하여 스케일을 통일시켜야한다. $$ \\bar{x_{ij}}=\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum^n_{i=1}(x_{ij}-\\bar{x}_j)^2}}$$\n",
    "  - 분모는 j번째 변수의 표준편차이며, 모든 표준화 변수는 1의 표준편차를 갖는다. 따라서 마지막 적합은 어떤 변수로 추정되던 스케일의 의존하지 않는다.\n",
    "- Ridge regression이 왜 least square보다 좋은가? **Bias-Variance Trade-Off**\n",
    "  - $\\lambda$ 증가에 따라 리지 적합 모델의 유연성이 감소해 variance가 줄고 bias가 커진다. least square는 $\\lambda$=0에 해당하여 variance가 높고 bias가 없다. $\\lambda$가 증가하면 리지 계수가 줄어들고 variance가 크게 감소하고 bias가 약간 커진다\n",
    "  - n<p 인 상황에서도 적용가능\n",
    "  - computational advantages : $\\lambda$가 고정되었을 때 ridge regression은 하나의 모델만 적합하며, 굉장히 빠르게 진행될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 The Lasso\n",
    "- 변수들의 부분집합을 포함하는 모델들을 일반적으로 선택하는 Best subset, forward stepwise, backward stepwise selection과 달리, ridge regression은 마지막 모델에 모든 p 변수들을 포함한다. \n",
    "- 페널티 항 $\\lambda\\sum\\beta^2_j$는 모든 변수를 0으로 수렴시키나, $\\lambda=\\inf$가 아니라면 모든 변수를 아예 0으로 만들지는 않을 것이다. \n",
    "  - 이 경우, 예측에는 문제가 없으나 해석에는 어려울 수 있따. \n",
    "- $\\hat\\beta^L$는 아래 식을 최소화 하는 값 $$ \\sum^n_{i=1}(y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij})^2+\\lambda\\sum^p_{j=1}|\\beta_j|=RSS+\\lambda\\sum^p_{j=1}|\\beta_j|, \\\\ |\\beta_j|:\\ lasso\\ penalty,\\quad ||\\beta||_1=\\sum|\\beta_j|$$\n",
    "  - lasso는 ridge와 달리, $\\lambda$가 충분히 클 경우, 계수 추정치를 0으로 만들어서 변수선택의 역할을 할 수 있게 한다.\n",
    "  - 따라서, $\\lambda$의 선택은 매우 중요하다\n",
    "- Formulation\n",
    "  $$ minimize_\\beta{\\sum^n_{i=1}(y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij})^2}\\\\ subject\\ to\\ \\sum^p_{j=1}|\\beta_j|\\leq s(lasso), \\sum^p_{j=1}\\beta^2_j \\leq s(ridge)$$\n",
    "  - p=2 일 때, $|\\beta_1|+|\\beta_2| \\leq\\ s(lasso),\\quad \\beta^2_1+\\beta^2_s \\leq\\ s(ridge) $\n",
    "  - lasso : $\\sum^p_{j=1}|\\beta_j|$의 양을 결정할 budget s의 제한 안에서 RSS가 가장 작은 계수 선택. s가 크면 단순히 최소제곱법이 되며, s가 작으면 budget을 어기는 것을 막기 위해 $\\sum^p_{j=1}|\\beta_j|$가 작아짐 ridge도 동일\n",
    "  - $$ minimize_\\beta{\\sum^n_{i=1}(y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij})^2} subject\\ to\\ \\sum^p_{j=1}I(\\beta_j\\neq0)\\leq s(best\\ subset\\ selection)$$\n",
    "- Lasso의 변수선택 성질\n",
    "  - ridge : 뾰족한 포인트가 없는 원형의 제약 -> axis에 교차점이 생기기 어려워 계수는 non-zero\n",
    "  - lasso : 뾰족한 꼭지점 형태의 제약 -> 조건이 꼭지점에 겹치면 계수가 0이 될 수 있음\n",
    "- Lasso & Ridge의 비교\n",
    "  - $\\lambda$가 커짐에 따라 variance가 줄고 bias가 증가한다는, 둘 모두 비슷한 움직임을 보임\n",
    "  - 비교적 적은 수의 변수가 큰 계수를 가질 때 lasso가 우위. 종속 변수가 많은 독립변수가 존재하여, 모든 계수들이 거의 비슷한 사이즈일 때 ridge가 우위\n",
    "- Ridge, Lasso의 베이지안 해석\n",
    "  - $\\beta$의 계수 벡터가 prior 분포가 있다고 가정함 : $p(\\beta)$, & 데이터의 Likelihood : $f(Y|X,\\beta)$\n",
    "  - prior 분포와 likelihood의 곱이 posterior 분포 $$ p(\\beta|X,Y) \\propto f(Y|X,\\beta)p(\\beta|X)=f(Y|X,\\beta)p(\\beta)$$\n",
    "  - 선형 모델 및 error는 독립적이며 정규분포임을 가정 + $p(\\beta)=\\prod^p_{j=1}g(\\beta_j),\\ g:density\\ function$\n",
    "    - g가 평균 0, 표준편차가 $\\lambda$의 함수인 표준편차의 정규분포 일 때, $\\beta$의 posterior mode가 ridge 회귀 솔루션\n",
    "    - g가 평균 0, 표준편차가 $\\lambda$의 함수인 표준편차의 double-exponential (Laplace)분포 일 때, $\\beta$의 posterior mode는 lasso 솔루션\n",
    "    - ridge, lasso는 $\\beta$의 simple prior 분포인 정규분포 에러의 선형모델\n",
    "    - lasso prior는 가파르게 0이 되고, ridge prior는 평탄하게 0으로 감 => lasso 많은 계수는 0, ridge 많은 계수는 0에 관하여 임의로 분포됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 튜닝 파라미너 선택\n",
    "- $\\lambda$ 및 $s$ 선택 \n",
    "  - Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Dimension Reduction Methods\n",
    "- dimension reduction medthods : 독립변수를 변환하여 변환 변수들을 통해 최소제곱 모델을 적합\n",
    "  $$ Z_m = \\sum^p_{j=1}\\phi_{jm}X_j,\\quad, y_i=\\theta_0+\\sum^M_{m=1}\\theta_mz_{im}+\\epsilon_i$$\n",
    "  - 줄어든 변수들을 통해서 계수 적합 : variance를 줄이는 효과\n",
    "  $$ \\sum^M_{m=1}\\theta_mz_{im}=\\sum^M_{m=1}\\theta_m\\sum^p_{j=1}\\phi_{jm}x_{ij}=\\sum^p_{j=1}\\sum^M_{m=1}\\theta_m\\phi_{jm}x_{ij}=\\sum^p_{j=1}\\beta_jx_{ij},\\\\ where\\ \\beta_j=\\sum^M_{m=1}\\theta_m\\phi_{jm}$$\n",
    "  - M=p, 일 때, 모든 $Z_m$는 선형독립이고, 제약조건이 없으며 최소제곱을 사용하는 것과 동일하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Principal Components Regression\n",
    "- Overview\n",
    "  - 가장 많이 변하는 데이터를 따르는 것이 가장 첫번째 principal component 방향이다\n",
    "  - 첫번째 principal component : 데이터와 가장 가까운 라인. 각 데이터와 라인의 수직 제곱 거리합(첫번째 라은으로의 각 데이터 투영)이 가장 최소화\n",
    "  - 두번째 principal component : 첫번째와 상관성이 없으며, 그 주에 가장 큰 분산을 차지 => 방향 수직\n",
    "- 회귀 접근\n",
    "  - 작은 수의 주 성분이라도 대부분 데이터의 변동성을 설명하기엔 충분 : 각 주성분은 종속변수와 관련된 대부분의 변동성을 나타냄. \n",
    "  - PCR의 가정 하, 주성분의 최소제곱 적합은, 주성분 자체가 대부분의 정보를 담고 있으며, 과적합을 완하하므로 그냥 회귀보다 더 나은 결과를 기대\n",
    "- 더 많은 주성분이 이용될 수록, bias가 늘고 variance가 늘어난다. 적합한 갯수의 주성분 이용 필요\n",
    "  - 주성분이 많이 필요한 복잡한 데이터에서 상대적으로 낮은 정확도, 첫 몇 개의 주성분이 충분한 설명력을 가질 때 높은 정확도를 보인다.\n",
    "- PCR이 데이터가 작을 경우 쓸 수 있는 좋은 방법이나, 변수 선택에는 좋은 방법이 아니다. ridge 보다는 lasso에 가까운 회귀\n",
    "- PCR 사용 시, 표준화 추천 : 스케일 통일되지 않으면 variance가 높은 변수가 주성분의 큰 부분을 차지할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Partial Least Squares\n",
    "- unsupervised 방법 : PCR은 데이터를 잘 설명하는 주성분이 종속변수 예측에 있어서도 잘 설명할 것인가에 대한 보장이 없는 것\n",
    "- Partial Least Squares : supervised way\n",
    "  - 기존 변수의 선형조합인 피쳐 생성 후 최소제곱법으로 새로운 선형 모델 적합한다 : 독립변수 설명할 뿐만 아니라 종속변수 적합과 관련시킴.\n",
    "  - 표준화 진행 -> 첫 피쳐 : $\\phi_{j1}$를 종속변수의 독립변수_1에 대한 간단 선형회귀의 계수로 설정 => $Z_1=\\sum^p_{j=1}\\phi_{j1}X_j$설정 시에 종속변수와 가장 강하게 연관성이 있는 것에 가장 큰 가중치 부여\n",
    "  - PCA보다 독립변수 설명에는 약할 수 있으나, 종속변수 설명에는 더 좋은 역할  \n",
    "  - 두번째 피쳐 -> 잔차를 이용(첫번째 주성분으로 설명되지 않은 정보) -> 첫번째 주성분 과정 같이 반복\n",
    "- 교차검증을 통해 주성분 갯수 결정. 표준화 필요\n",
    "- bias 줄이고 variance 증가시킬 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Considerations in High Dimensions\n",
    "### 6.4.1 High-Dimensional Data\n",
    "- 변수의 수는 기술의 발달로 제한없이 증가할 수 있으나, 비용, 가능성 등에 의해서 관칙치 수는 부족할 수 있다. => high dimensional : bias-variance tradeoff / overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 What Goes Wrong in High Dimension?\n",
    "- 고차원 데이터에서 기존 최소제곱 회귀를 사용할 경우\n",
    "  - 사용 불가능 : 잔차가 0인 완벽한 데이터 적합을 나타냄, 오버피팅\n",
    "  - 관련없는 피쳐가 추가되어도 훈련데이터 MSE가 0까지 줄어든다 : 피쳐 추가가 계수 추정에 variance를 매우 크게 일으키므로\n",
    "  - $C_p,\\ AIC,\\ BIC$도 고차원 데이터에는 부적합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 Regression in High Dimensions\n",
    "- 덜 유연한 모델이 필요 : Ridge, lasso, PCR, forward stepwise\n",
    "  - regularization or shrinkage\n",
    "  - 적절한 파라미터 선택\n",
    "  - 관련 없는 추가 데이터가 추가도리 수록 시험 에러가 커짐\n",
    "- 차원의 저주 : 노이즈 피쳐가 오버피팅을 악화시킬 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.4 Interpreting Results in High Dimensions\n",
    "- multicollinearity : 어떤 변수가 예측에 가장 도움이 되는지 구분할 수 없다\n",
    "- 훈련 데이터의 결과로 모델의 성능을 평가해서는 안됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Lab : Linear Models and Regularization Methods\n",
    "### 6.5.1 Subset Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>446</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "0    293    66      1    30   29     14      1     293     66       1     30   \n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "\n",
       "   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n",
       "0    29      14      A        E      446       33      20     NaN         A  \n",
       "1   414     375      N        W      632       43      10   475.0         N  \n",
       "2   266     263      A        W      880       82      14   480.0         A  \n",
       "3   838     354      N        E      200       11       3   500.0         N  \n",
       "4    46      33      N        E      805       40       4    91.5         N  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"../data/Hitters.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 322 entries, 0 to 321\n",
      "Data columns (total 20 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   AtBat      322 non-null    int64  \n",
      " 1   Hits       322 non-null    int64  \n",
      " 2   HmRun      322 non-null    int64  \n",
      " 3   Runs       322 non-null    int64  \n",
      " 4   RBI        322 non-null    int64  \n",
      " 5   Walks      322 non-null    int64  \n",
      " 6   Years      322 non-null    int64  \n",
      " 7   CAtBat     322 non-null    int64  \n",
      " 8   CHits      322 non-null    int64  \n",
      " 9   CHmRun     322 non-null    int64  \n",
      " 10  CRuns      322 non-null    int64  \n",
      " 11  CRBI       322 non-null    int64  \n",
      " 12  CWalks     322 non-null    int64  \n",
      " 13  League     322 non-null    object \n",
      " 14  Division   322 non-null    object \n",
      " 15  PutOuts    322 non-null    int64  \n",
      " 16  Assists    322 non-null    int64  \n",
      " 17  Errors     322 non-null    int64  \n",
      " 18  Salary     263 non-null    float64\n",
      " 19  NewLeague  322 non-null    object \n",
      "dtypes: float64(1), int64(16), object(3)\n",
      "memory usage: 50.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 263 entries, 1 to 321\n",
      "Data columns (total 20 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   AtBat      263 non-null    int64  \n",
      " 1   Hits       263 non-null    int64  \n",
      " 2   HmRun      263 non-null    int64  \n",
      " 3   Runs       263 non-null    int64  \n",
      " 4   RBI        263 non-null    int64  \n",
      " 5   Walks      263 non-null    int64  \n",
      " 6   Years      263 non-null    int64  \n",
      " 7   CAtBat     263 non-null    int64  \n",
      " 8   CHits      263 non-null    int64  \n",
      " 9   CHmRun     263 non-null    int64  \n",
      " 10  CRuns      263 non-null    int64  \n",
      " 11  CRBI       263 non-null    int64  \n",
      " 12  CWalks     263 non-null    int64  \n",
      " 13  League     263 non-null    object \n",
      " 14  Division   263 non-null    object \n",
      " 15  PutOuts    263 non-null    int64  \n",
      " 16  Assists    263 non-null    int64  \n",
      " 17  Errors     263 non-null    int64  \n",
      " 18  Salary     263 non-null    float64\n",
      " 19  NewLeague  263 non-null    object \n",
      "dtypes: float64(1), int64(16), object(3)\n",
      "memory usage: 43.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.dropna(axis=0, inplace=True) # remove all data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best subset selection method\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "def process_subset(X,y,feature_set):\n",
    "    model = sm.OLS(y,X[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    AIC = regr.aic\n",
    "    return {'model':regr, 'AIC':AIC}\n",
    "\n",
    "def forward(X, y, predictors):\n",
    "    tic = time.time()\n",
    "\n",
    "    # 변수들이 미리 정의된 predictors에 있는지 확인 후 분류\n",
    "    remaining_predictors = [p for p in X.xolumns.difference(['intercept']) if p not in predictors]\n",
    "    results = []\n",
    "    for p in remaining_predictors:\n",
    "        results.append(process_subset(X=X, y=y, feature_set=predictors+[p]+['intercept']))\n",
    "\n",
    "    models = pd.DataFrame(results)\n",
    "    best_model = models.loc[models['AIC'].argmin()] # index\n",
    "    toc = time.time()\n",
    "\n",
    "    print(f\"processed {models.shape[0]}, models on {len(predictors)+1}, predictors in {toc - tic}\")\n",
    "    print(f\"Selected predictors: {best_model['model'].model.exog_names}, AIC : {best_model[0]}\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def backward(X, y, predictors):\n",
    "    tic = time.time()\n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(predictors, len(predictors)-1):\n",
    "        results.append(process_subset(X=X, y=y, feature_set=list(combo)+['intercept']))\n",
    "\n",
    "    models = pd.DataFrame(results)\n",
    "\n",
    "    best_model = models.loc[models['AIC'].argmin()] # index\n",
    "    toc = time.time()\n",
    "\n",
    "    print(f\"processed {models.shape[0]}, models on {len(predictors)+1}, predictors in {toc-tic}\")\n",
    "    print(f\"Selected predictors: {best_model['model'].model.exog_names}, AIC : {best_model[0]}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def stepwise_model(X, y):\n",
    "    stepmodels = pd.DataFrame(columns=[\"AIC\", \"model\"])\n",
    "    tic = time.time()\n",
    "    predictors = []\n",
    "    smodel_before = process_subset(X,y,predictors+{'intercept'})['AIC']\n",
    "\n",
    "    for i in range(1, len(X.columns.difference(['intercept']))+1):\n",
    "        forward_result = forward(X,y, predictors=predictors) # constant added\n",
    "        print(\"forward\")\n",
    "        stepmodels.loc[i] = forward_result\n",
    "        predictors = stepmodels.loc[i][\"model\"].model.exog_names\n",
    "        backward_result = backward(X, y, predictors=predictors)\n",
    "        if backward_result['AIC'] < forward_result['AIC']:\n",
    "            stepmodels.loc[i] = backward_result\n",
    "            predictors = stepmodels.loc[i][\"model\"].model.exog_names\n",
    "            smodel_before = stepmodels.loc[i][\"AIC\"]\n",
    "            predictors = [k for k in predictors if k != 'intercept']\n",
    "            print('backward')\n",
    "        if stepmodels.loc[i]['AIC'] > smodel_before:\n",
    "            break\n",
    "        else:\n",
    "            smodel_before = stepmodels.loc[i][\"AIC\"]\n",
    "    toc = time.time()\n",
    "    print(f\"Elapsed time: {toc-tic}, seconds\")\n",
    "    \n",
    "    return (stepmodels['model'][len(stepmodels['model'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['intercept'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6. Linear Model Selection and Regularization.ipynb 셀 22\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(labels\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mSalary\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mSalary\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m backward_model \u001b[39m=\u001b[39m backward(X, y, X\u001b[39m.\u001b[39;49mcolumns)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m backward_model[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msummary()\n",
      "\u001b[1;32m/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6. Linear Model Selection and Regularization.ipynb 셀 22\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(X, y, predictors)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m combo \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcombinations(predictors, \u001b[39mlen\u001b[39m(predictors)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     results\u001b[39m.\u001b[39mappend(process_subset(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, feature_set\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(combo)\u001b[39m+\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mintercept\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m models \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(results)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m best_model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mloc[models[\u001b[39m'\u001b[39m\u001b[39mAIC\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39margmin()] \u001b[39m# index\u001b[39;00m\n",
      "\u001b[1;32m/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6. Linear Model Selection and Regularization.ipynb 셀 22\u001b[0m in \u001b[0;36mprocess_subset\u001b[0;34m(X, y, feature_set)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_subset\u001b[39m(X,y,feature_set):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39mOLS(y,X[\u001b[39mlist\u001b[39;49m(feature_set)])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     regr \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/klee30810/Documents/Career/TIL/ISLR_python/notebooks/6.%20Linear%20Model%20Selection%20and%20Regularization.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     AIC \u001b[39m=\u001b[39m regr\u001b[39m.\u001b[39maic\n",
      "File \u001b[0;32m~/miniconda3/envs/kmlee/lib/python3.8/site-packages/pandas/core/frame.py:3464\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3463\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3464\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3466\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/kmlee/lib/python3.8/site-packages/pandas/core/indexing.py:1314\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 1314\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_read_indexer(keyarr, indexer, axis)\n\u001b[1;32m   1316\u001b[0m \u001b[39mif\u001b[39;00m needs_i8_conversion(ax\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   1317\u001b[0m     ax, (IntervalIndex, CategoricalIndex)\n\u001b[1;32m   1318\u001b[0m ):\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# For CategoricalIndex take instead of reindex to preserve dtype.\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m     \u001b[39m#  For IntervalIndex this is to map integers to the Intervals they match to.\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m     keyarr \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39mtake(indexer)\n",
      "File \u001b[0;32m~/miniconda3/envs/kmlee/lib/python3.8/site-packages/pandas/core/indexing.py:1377\u001b[0m, in \u001b[0;36m_LocIndexer._validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1376\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 1377\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['intercept'] not in index\""
     ]
    }
   ],
   "source": [
    "X = df.drop(labels=['Salary'], axis=1)\n",
    "y = df['Salary']\n",
    "\n",
    "backward_model = backward(X, y, X.columns)\n",
    "backward_model[0].summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kmlee')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d50b53f7eecc192699096c3b322d2e418f52a0f23d06ad75ff9fc286d66daf3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
