{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear model을 Least square fitting이 아닌 다른 방법을 통해서 개선해보자\n",
    "  - 예측 정확도 : 독립, 종속변수가 선형이라고 가정할때, least square method는 낮은 bias일 것이다. 변수 종류보다 데이터수가 많다며 더 낮은 variance를 만들고 평가 정확도도 좋을 것이다.\n",
    "    - 만약 데이터의 수가 많지 않다면 least square fit에 variance가 많아져, 훈련되지 않은 평가 데이터셋에서 좋지 않은 결과를 만들 것이다. 만약 변수의 수가 데이터의 수보다 많다면 유일한 least square 계수가 없으므로 무한대의 Variance가 생긴다.\n",
    "    - 추정된 계수를 constrain or shrink를 통해서 사소한 bias의 증가와 함께 큰 variance의 감소를 만들 수 있으며, 평가 정확도를 크게 개선할 수 있다\n",
    "  - 모델 해석도 : 몇몇 독립변수들은 종속변수에 상관이 없을 수도 있고 이 들은 모델 복잡도만 크게 한다. 이런 변수를 지워서(해당 계수 0으로) 모델의 해석도를 높일 수 있다.\n",
    "    - least square는 어떤 계수 추정치를 0으로 만들지는 못한다. 변수 선택을 통해 관련 없는 독립변수를 제외해보자\n",
    "- least square 대안 들\n",
    "  - subset selection : p 독립변수에 대한 종속변수와 연관되어 있다고 생각되는 부분집합. 줄어든 부분집합 변수를 least square\n",
    "  - shrinkage : 모든 독립변수들을 모델 적합에 사용. least square 추정치에 따라서 추정계수가 0으로 수렴. 변수 선택에도 사용가능\n",
    "  - dimension reduction : p개의 변수들으 M차원 서브공간으로 투영하는 것. M개의 다른 선형 조합 or 투영 진행. M개의 투영들이 least square에 의해 선형 회귀모델 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Subset Selection\n",
    "### 6.1.1 Best Subset Selection\n",
    "- best subset selection : p개의 독립변수에 대한 가능한 각 조합에 least square 회귀 진행 -> p개의 하나의 독립변수만 가진 모델, $\\binom{p}{2}=p(p-1)/2$개의 2개의 독립변수를 가진 모델들 etc & 어떤 것이 제일 나은 모델인지 확인\n",
    "  - $2^p$개의 가능성이 있어서 개수가 너무 많다\n",
    "> Algorithm 6.1 Best Subset Selection\n",
    "> 1. $M_0$를 null model로 가정(아무 독립변수 없음, 각 관측에 대한 표본 평균만 예측)\n",
    "> 2. $k=1,2,...,p:\\\\$\n",
    ">  a) 모든 k개의 독립변수를 포함하는 $\\binom{p}{k}$를 적합  $\\\\$\n",
    ">  b) 이중 가장 낮은 RSS or 높은 $R^2$를 가진 모델을 $M_k$라고 지정\n",
    "> 3. Cross validation, AIC, BIC, adjusted $R^2$등을 통해서 가장 좋은 모델을 확인\n",
    "  - $R^2$는 독립변수의 수가 늘어나면 자연스럽게 늘게 되는데 이건 훈련 오차이다. 훈련오차는 평가 오차보다 작을 수 밖에 없으며, 훈련 오차가 작은 것이 평가 오차가 작은 것을 보장하지않으므로, CV, $C_p$, BIC, adjusted $R^2$ 등을 이용한다.\n",
    "  - 다른 회귀에도 사용가능 : logistic regression에서는 RSS 대신에 deviance($-2 * maximized\\ log-likelihood$)를 사용 \n",
    "  - 독립변수 증가에 따라 계산량이 너무 많아짐 ($2^p$) -> 변수가 40개가 넘어가면 못함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Stepwise Selection\n",
    "- 찾아야 되는 공간이 클 수록, 훈련 데이터에만 적합한 모델을 찾을 가능성이 높아진다. => 더 제한된 데이터셋에서 적합하기\n",
    "- Forward Stepwise Selection\n",
    "  - 같은 $2^p$의 가능성을 가지나 더 적은 셋\n",
    "  - 변수가 없는 모델에서 시작해서 한번에 하나씩 더하여 모든 변수가 있는 모델까지 적합\n",
    "  - 매 단계에서 적합에 추가적인 이득을 주는 변수만 포함한다\n",
    "> Algorithm 6.2 Forward stepwise selection\n",
    "> 1. $M_0$를 null model로 가정(아무 독립변수 없음, 각 관측에 대한 표본 평균만 예측)\n",
    "> 2. $k=0,1,...,p-1:\\\\$\n",
    ">  a) 한 개의 추가적인 변수가 있는 $M_k$가 증가된 p-k개의 모든 모델을 고려 $\\\\$\n",
    ">  b) p-k개의 모델 중 가장 낮은 RSS or 높은 $R^2$를 가진 모델을 $M_{k+1}$라고 지정\n",
    "> 3. Cross validation, AIC, BIC, adjusted $R^2$등을 통해서 가장 좋은 모델을 확인\n",
    "  - 하나의 null model에서 k번째 반복에 해당되는 p-k개의 모델을 적합 : $\\sum^{p-1}_{k=0}(p=k)=1+p(p+1)/2$\n",
    "  - 실패 가능 ex. 일변수 모델에서는 첫 번째, 이변수 모델에서는 두 세번째 변수가 채택되었을때, 이변수 모델에서는 첫 번재 변수가 무조건 선택되어야 하므로, 두 세번째 변수만 포함 된 모델을 선택할 수 없다\n",
    "  - 변수(p)가 데이터(n)보다 많은 고차원 데이터에도 적용될 수 있으나,least square를 사용해서 $M_0,...,M_{n-1}$ n개의 부분모델 밖에서 만들 수 없다\n",
    "- Backward Stepwise Selection\n",
    "  - p 변수 모두 포함된 least square 모델에서 반복적으로 하나씩 가장 무의미한 변수를 지워나감\n",
    "> Algorithm 6.3 Backward stepwise selection\n",
    "> 1. $M_p$를 full model로 가정(모든 변수 가짐)\n",
    "> 2. $k=p,p-1,...,1:\\\\$\n",
    ">  a) k-1개의 모든 변수들 중, $M_k$의 모든 변수들 중 하나 만 없는 모든 k개의 모델을 고려  $\\\\$\n",
    ">  b) k개의 모델 중 가장 낮은 RSS or 높은 $R^2$를 가진 모델을 $M_{k-1}$라고 지정\n",
    "> 3. Cross validation, AIC, BIC, adjusted $R^2$등을 통해서 가장 좋은 모델을 확인\n",
    "  - Backward selection도 1+p(p+1)/2개의 모델을 가져, p가 매우 커도 적용이 가능\n",
    "  - forward처럼 backward도 최고의 조합을 보장하지는 않는다.\n",
    "  - backward는 데이터의 수가 변수의 수 보다 많아야 하나(full model이 적합되어야 하므로), forward는 데이터의 수가 더 적어도 적용가능\n",
    "- Hybrid Approaches\n",
    "  - 순차적으로 변수를 추가한 후, 적합에 기여하지 않는 변수는 지워나감."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Choosing the Optimal Model\n",
    "- 모든 독립변수를 가진 모델이 당연히 좋은 RSS, $R^2$를 가질 것이고, 이는 훈련 오차와 관련된 값이라 좋은 모델 평가값이라고 할 수 없다\n",
    "  1) 오버피팅으로 인해 bias를 설명할 수 있는 훈련오차에 보정을 만듬으로써 간접적으로 평가 오차 추정\n",
    "  2) 검증 세트나, cross-validation을 통해 평가 오차를 직접적으로 적용\n",
    "- $C_p$, AIC, BIC, and Adjusted $R^2$\n",
    "  - MSE : least square를 통한 훈련 데이터 적합으로 인해 훈련 데이터 MSE는 평가 데이터 MSE의 과소평가 버전이며, 변수가 증가할 수록 훈련 오차는 줄어드는데 평가 오차는 늘 수 있어 훈련 데이터에 대한 RSS, $R^2$는 평가에 이용될 수 없음\n",
    "  - 모델 사이즈에 대한 훈련 오차를 보정하는 여러 기법들\n",
    "    - $C_p$ : $2d\\hat\\sigma^2$의 페널티를 훈련 RSS에 더함으로서, 훈련 오차가 평가 오차에 대한 과소평가 되는 부분을 보완 $$C_p=\\frac{1}{n}(RSS+2d\\hat\\sigma^2),\\quad \\hat\\sigma^2:error\\ \\epsilon 에\\ 대한\\ 분산\\ 추정치(보통\\ full\\ model) $$\n",
    "      - 변수의 수가 늘어나면 페널티도 늘어나면서 훈련 오차 RSS 감소 보정\n",
    "    - AIC (Akaike Information Criterion) : 최대우도적합을 통한 큰 클래스 모델 적합 $$AIC = \\frac{1}{n}(RSS+2d\\hat\\sigma^2) $$\n",
    "      - $C_p$와 정비례\n",
    "    - BIC (Bayesian Information Criterion) : 베이지안 관점 해설 $$BIC=\\frac{1}{n}(RSS+log(n)d\\hat\\sigma^2) $$\n",
    "      - 모든 n>7에 대해 logn>2 이므로, BIC는 많은 변수가 있는 모델에 더 큰 페널티를 주어 $C_p$보다 더 작은 모델들을 결과로 만듬\n",
    "    - Adjusted $R^2$ : 모든 옳은 변수가 모델에 포함 될 시, 추가적인 노이즈 변수는 약간의 RSS 감소를 만듬. $$ Adjusted\\ R^2=1-\\frac{RSS/(n-d-1)}{TSS/(n-1)}$$\n",
    "      - 노이즈 변수 추가는 d를 증가시켜, adjusted $R^2$를 줄임\n",
    "- Validation and Cross-Validation\n",
    "  - 검증 데이터 셋을 통한 추정 평가 결과로 모델 평가\n",
    "  - one-standard-error rule : 오차가 비슷한 모델 중에서는 변수가 적은 모델이 좋다\n",
    "  1) 각 모델에 대한 추정 평가 MSE의 표준오차 계산\n",
    "  2) 표준오차 1 수준내 커브의 가장 낮은 포인트에서 추정 평가오차가 가장 낮은 모델 선택\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Shrinkage Methods\n",
    "- 모든 변수를 사용하고, 추정 계수를 제한하거나 0으로 줄여서 적합\n",
    "- 적합도 향상에 명확하지는 않지만 매우 급격히 variance를 줄여준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Ridge Regression\n",
    "$$ RSS = \\sum^n_{i=1}(y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij})^2$$\n",
    "- Ridge Regression : least square와 비슷한데, 계수에 약간의 차이를 최소화 함\n",
    "  - $\\hat\\beta^R$는 아래 식을 최소화 하는 값 $$ \\sum^n_{i=1}(y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij})^2+\\lambda\\sum^p_{j=1}\\beta^2_j=RSS+\\lambda\\sum^p_{j=1}\\beta^2_j, \\\\ \\lambda:tuning\\ parameter$$\n",
    "  - RSS를 최소하하여 데이터 적합을 시킴과 동시에 shirinkage penalty $\\lambda\\sum_j\\beta^2_j$는 각 계수가 0일 때 작아져, 각 계수를 0으로 수렴시킴 \n",
    "  - $\\lambda$는 회귀 계수 추정에 있어 두 항의 상대적 영향력 결정\n",
    "  - $\\lambda$ 선택이 중요\n",
    "  - shrinkage penalty는 intercept에 적용되지는 않음(모든 변수가 0일 때 평균)\n",
    "  - $\\lambda$가 클 수록 모든 리지 계수가 0으로 수렴하면서, 변수가 없는 null model화가 됨\n",
    "  - $||\\hat\\beta^R_2||_2/||\\hat\\beta||_2,\\ ||\\beta||_2=\\sqrt{\\sum^p_{j=1}\\beta^2_j}:l2\\ norm,$ 0~$\\beta$거리\n",
    "    - $\\lambda$가 커질 수록 $\\hat\\beta^R_\\lambda$의 l2 norm은 줄어들며, $||\\hat\\beta^R_2||_2/||\\hat\\beta||_2$도 줄어들음\n",
    "    - $||\\hat\\beta^R_2||_2/||\\hat\\beta||_2$이 1일때 $\\lambda$=0(리지 회귀 계수가 least square 추정치와 같고, l2 norm도 같음), $||\\hat\\beta^R_2||_2/||\\hat\\beta||_2$이 0일때 $\\lambda=\\inf$(리지 회귀 계수가 0벡터, l2 norm도 0)\n",
    "- 표준 최소제곱 계수 추정치는 scale equivariant : $X_j$를 c만큼 곱하면, 최소제곱계수 추정치가 1/c만큼 스케일링 되어 $X_j\\hat\\beta_j$는 동일하다\n",
    "- 반면에 리지 회귀 추정치는 상수를 각 변수에 곱해주었을 때 급격하게 변할 수 있다 => $X_j\\hat\\beta^R_{j,\\lambda}$는 $\\lambda$ 뿐만 아니라 j번ㅉ ㅐ변수의 스케일링에도 의존한다. $X_j\\hat\\beta^R_{j,\\lambda}$는 다른 변수의 스케일링에도 의존할 수 있어서 ridge regression은 변수 표준화를 하여 스케일을 통일시켜야한다. $$ \\bar{x_{ij}}=\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum^n_{i=1}(x_{ij}-\\bar{x}_j)^2}}$$\n",
    "  - 분모는 j번째 변수의 표준편차이며, 모든 표준화 변수는 1의 표준편차를 갖는다. 따라서 마지막 적합은 어떤 변수로 추정되던 스케일의 의존하지 않는다.\n",
    "- Ridge regression이 왜 least square보다 좋은가? **Bias-Variance Trade-Off**\n",
    "  - $\\lambda$ 증가에 따라 리지 적합 모델의 유연성이 감소해 variance가 줄고 bias가 커진다. least square는 $\\lambda$=0에 해당하여 variance가 높고 bias가 없다. $\\lambda$가 증가하면 리지 계수가 줄어들고 variance가 크게 감소하고 bias가 약간 커진다\n",
    "  - n<p 인 상황에서도 적용가능\n",
    "  - computational advantages : $\\lambda$가 고정되었을 때 ridge regression은 하나의 모델만 적합하며, 굉장히 빠르게 진행될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 The Lasso\n",
    "- 변수들의 부분집합을 포함하는 모델들을 일반적으로 선택하는 Best subset, forward stepwise, backward stepwise selection과 달리, ridge regression은 마지막 모델에 모든 p 변수들을 포함한다. \n",
    "- 페널티 항 $\\lambda\\sum\\beta^2_j$는 모든 변수를 0으로 수렴시키나, $\\lambda=\\inf$가 아니라면 모든 변수를 아예 0으로 만들지는 않을 것이다. \n",
    "  - 이 경우, 예측에는 문제가 없으나 해석에는 어려울 수 있따. \n",
    "- $\\hat\\beta^L$는 아래 식을 최소화 하는 값 $$ \\sum^n_{i=1}(y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij})^2+\\lambda\\sum^p_{j=1}|\\beta_j|=RSS+\\lambda\\sum^p_{j=1}|\\beta_j|, \\\\ |\\beta_j|:\\ lasso\\ penalty,\\quad ||\\beta||_1=\\sum|\\beta_j|$$\n",
    "  - lasso는 ridge와 달리, $\\lambda$가 충분히 클 경우, 계수 추정치를 0으로 만들어서 변수선택의 역할을 할 수 있게 한다.\n",
    "  - 따라서, $\\lambda$의 선택은 매우 중요하다\n",
    "- Formulation\n",
    "  $$ minimize_\\beta{\\sum^n_{i=1}(y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij})^2}\\\\ subject\\ to\\ \\sum^p_{j=1}|\\beta_j|\\leq s(lasso), \\sum^p_{j=1}\\beta^2_j \\leq s(ridge)$$\n",
    "  - p=2 일 때, $|\\beta_1|+|\\beta_2| \\leq\\ s(lasso),\\quad \\beta^2_1+\\beta^2_s \\leq\\ s(ridge) $\n",
    "  - lasso : $\\sum^p_{j=1}|\\beta_j|$의 양을 결정할 budget s의 제한 안에서 RSS가 가장 작은 계수 선택. s가 크면 단순히 최소제곱법이 되며, s가 작으면 budget을 어기는 것을 막기 위해 $\\sum^p_{j=1}|\\beta_j|$가 작아짐 ridge도 동일\n",
    "  - $$ minimize_\\beta{\\sum^n_{i=1}(y_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij})^2} subject\\ to\\ \\sum^p_{j=1}I(\\beta_j\\neq0)\\leq s(best\\ subset\\ selection)$$\n",
    "- Lasso의 변수선택 성질\n",
    "  - ridge : 뾰족한 포인트가 없는 원형의 제약 -> axis에 교차점이 생기기 어려워 계수는 non-zero\n",
    "  - lasso : 뾰족한 꼭지점 형태의 제약 -> 조건이 꼭지점에 겹치면 계수가 0이 될 수 있음\n",
    "- Lasso & Ridge의 비교\n",
    "  - $\\lambda$가 커짐에 따라 variance가 줄고 bias가 증가한다는, 둘 모두 비슷한 움직임을 보임\n",
    "  - 비교적 적은 수의 변수가 큰 계수를 가질 때 lasso가 우위. 종속 변수가 많은 독립변수가 존재하여, 모든 계수들이 거의 비슷한 사이즈일 때 ridge가 우위\n",
    "- Ridge, Lasso의 베이지안 해석\n",
    "  - $\\beta$의 계수 벡터가 prior 분포가 있다고 가정함 : $p(\\beta)$, & 데이터의 Likelihood : $f(Y|X,\\beta)$\n",
    "  - prior 분포와 likelihood의 곱이 posterior 분포 $$ p(\\beta|X,Y) \\propto f(Y|X,\\beta)p(\\beta|X)=f(Y|X,\\beta)p(\\beta)$$\n",
    "  - 선형 모델 및 error는 독립적이며 정규분포임을 가정 + $p(\\beta)=\\prod^p_{j=1}g(\\beta_j),\\ g:density\\ function$\n",
    "    - g가 평균 0, 표준편차가 $\\lambda$의 함수인 표준편차의 정규분포 일 때, $\\beta$의 posterior mode가 ridge 회귀 솔루션\n",
    "    - g가 평균 0, 표준편차가 $\\lambda$의 함수인 표준편차의 double-exponential (Laplace)분포 일 때, $\\beta$의 posterior mode는 lasso 솔루션\n",
    "    - ridge, lasso는 $\\beta$의 simple prior 분포인 정규분포 에러의 선형모델\n",
    "    - lasso prior는 가파르게 0이 되고, ridge prior는 평탄하게 0으로 감 => lasso 많은 계수는 0, ridge 많은 계수는 0에 관하여 임의로 분포됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 튜닝 파라미너 선택\n",
    "- $\\lambda$ 및 $s$ 선택 \n",
    "  - Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Dimension Reduction Methods\n",
    "- dimension reduction medthods : 독립변수를 변환하여 변환 변수들을 통해 최소제곱 모델을 적합\n",
    "  $$ Z_m = \\sum^p_{j=1}\\phi_{jm}X_j,\\quad, y_i=\\theta_0+\\sum^M_{m=1}\\theta_mz_{im}+\\epsilon_i$$\n",
    "  - 줄어든 변수들을 통해서 계수 적합 : variance를 줄이는 효과\n",
    "  $$ \\sum^M_{m=1}\\theta_mz_{im}=\\sum^M_{m=1}\\theta_m\\sum^p_{j=1}\\phi_{jm}x_{ij}=\\sum^p_{j=1}\\sum^M_{m=1}\\theta_m\\phi_{jm}x_{ij}=\\sum^p_{j=1}\\beta_jx_{ij},\\\\ where\\ \\beta_j=\\sum^M_{m=1}\\theta_m\\phi_{jm}$$\n",
    "  - M=p, 일 때, 모든 $Z_m$는 선형독립이고, 제약조건이 없으며 최소제곱을 사용하는 것과 동일하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Principal Components Regression\n",
    "- Overview\n",
    "  - 가장 많이 변하는 데이터를 따르는 것이 가장 첫번째 principal component 방향이다\n",
    "  - 첫번째 principal component : 데이터와 가장 가까운 라인. 각 데이터와 라인의 수직 제곱 거리합(첫번째 라은으로의 각 데이터 투영)이 가장 최소화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
